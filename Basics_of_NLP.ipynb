{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some famous speech by Martin Luther King\n",
    "Para=\"\"\"I have a dream that one day down in Alabama, with its vicious racists, with its governor having his lips dripping with the words of interposition and nullification – one day right there in Alabama little black boys and black girls will be able to join hands with little white boys and white girls as sisters and brothers.\n",
    "\n",
    "    I have a dream today.\n",
    "\n",
    "    I have a dream that one day every valley shall be exalted and every hill and mountain shall be made low, the rough places will be made plain, and the crooked places will be made straight, and the glory of the Lord shall be revealed and all flesh shall see it together.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now using the sentence tokenizer, we can split the whole paragraph into multiple sentences, which are split based on the new lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=nltk.sent_tokenize(Para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we have 3 sentences from the above paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I have a dream that one day down in Alabama, with its vicious racists, with its governor having his lips dripping with the words of interposition and nullification – one day right there in Alabama little black boys and black girls will be able to join hands with little white boys and white girls as sisters and brothers.',\n",
       " 'I have a dream today.',\n",
       " 'I have a dream that one day every valley shall be exalted and every hill and mountain shall be made low, the rough places will be made plain, and the crooked places will be made straight, and the glory of the Lord shall be revealed and all flesh shall see it together.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we are done with the sentence tokenization, Let's start the word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the word tokenizer and stopwords library\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Create a Stemmer object for Stemming\n",
    "stemmer=PorterStemmer()\n",
    "\n",
    "#Since we have 3 sentences, lets tak each of the sentences\n",
    "for i in range(len(sentence)):\n",
    "    #Tokenize the words, so that you get the individual words in a list\n",
    "    words_non_stem=word_tokenize(sentence[i])\n",
    "    #Now take each of the tokenized words of each sentence and then apply :\n",
    "    #1. Stop words removal\n",
    "    #2. Stem each word and store it in a \"words\" list\n",
    "    words_stem=[stemmer.stem(word) for word in words_non_stem if word not in stopwords.words('english') ]\n",
    "    sentence[i]=' '.join(words_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I dream one day alabama , viciou racist , governor lip drip word interposit nullif – one day right alabama littl black boy black girl abl join hand littl white boy white girl sister brother .',\n",
       " 'I dream today .',\n",
       " 'I dream one day everi valley shall exalt everi hill mountain shall made low , rough place made plain , crook place made straight , glori lord shall reveal flesh shall see togeth .']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we can see from above Stemmed words using Porter Stemmer, which basically works on removing the suffix of a given word. Let's see how the same above sentences are now stemmed using Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lancaster=LancasterStemmer()\n",
    "\n",
    "for i in range(len(sentence)):\n",
    "    words_Lanc=word_tokenize(sentence[i])\n",
    "    words_Lanc=[Lancaster.stem(word) for word in words_Lanc if word not in stopwords.words('english')]\n",
    "    sentence[i]=' '.join(words_Lanc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have seen Stemming, Let's see Lemmetization and see how that improves the grammer better than the Lemmetization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "Lemma=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1=nltk.sent_tokenize(Para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentence1)):\n",
    "    words_lem=word_tokenize(sentence1[i])\n",
    "    words_lem = [ Lemma.lemmatize(word) for word in words_lem if word not in stopwords.words('english')]\n",
    "    sentence1[i]=' '.join(words_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I dream one day Alabama , vicious racist , governor lip dripping word interposition nullification – one day right Alabama little black boy black girl able join hand little white boy white girl sister brother .',\n",
       " 'I dream today .',\n",
       " 'I dream one day every valley shall exalted every hill mountain shall made low , rough place made plain , crooked place made straight , glory Lord shall revealed flesh shall see together .']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I dream one day alabama , viciou racist , governor lip drip word interposit nullif – one day right alabama littl black boy black girl abl join hand littl white boy white girl sister brother .', 'I dream today .', 'I dream one day everi valley shall exalt everi hill mountain shall made low , rough place made plain , crook place made straight , glori lord shall reveal flesh shall see togeth .']\n",
      "############################################################\n",
      "############################################################\n",
      "['I dream one day Alabama , vicious racist , governor lip dripping word interposition nullification – one day right Alabama little black boy black girl able join hand little white boy white girl sister brother .', 'I dream today .', 'I dream one day every valley shall exalted every hill mountain shall made low , rough place made plain , crooked place made straight , glory Lord shall revealed flesh shall see together .']\n"
     ]
    }
   ],
   "source": [
    "print(sentence)\n",
    "print(\"############################################################\")\n",
    "print(\"############################################################\")\n",
    "print(sentence1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's start and analyze the Bag of words and Count Vectorizer method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2=sent_tokenize(Para)\n",
    "\n",
    "#Tokenize the words\n",
    "for i in range(len(sentence2)):\n",
    "    words=word_tokenize(sentence2[i])\n",
    "    \n",
    "#Remove punctuations and convert all the words to small/lower case\n",
    "\n",
    "for i in range(len(words)):\n",
    "    words[i]=words[i].lower()\n",
    "    if words[i] in string.punctuation:\n",
    "        words[i]=words[i].replace(words[i],'')\n",
    "\n",
    "#Remove stop words and also the do a stemming\n",
    "words=[stemmer.stem(word) for word in words if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dream': 2,\n",
       " 'one': 12,\n",
       " 'day': 1,\n",
       " 'everi': 3,\n",
       " 'valley': 21,\n",
       " 'shall': 18,\n",
       " 'exalt': 4,\n",
       " 'hill': 7,\n",
       " 'mountain': 11,\n",
       " 'made': 10,\n",
       " 'low': 9,\n",
       " 'rough': 16,\n",
       " 'place': 13,\n",
       " 'plain': 14,\n",
       " 'crook': 0,\n",
       " 'straight': 19,\n",
       " 'glori': 6,\n",
       " 'lord': 8,\n",
       " 'reveal': 15,\n",
       " 'flesh': 5,\n",
       " 'see': 17,\n",
       " 'togeth': 20}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now you could see that the CV_Words would basically create a 33x22 matrix, with each words in rows(33) and only unique words in columns(22)\n",
    "cv=CountVectorizer()\n",
    "CV_Words=cv.fit_transform(words).toarray()\n",
    "#The vocabulary is the unique set of words in the corpus/sentence. The syntax is \"word:index\"\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pr=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tast'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.stem('tastes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tasti'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.stem('tasty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'troubl'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.stem('troubles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trouble'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('troubling',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trouble'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('troubles',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tasty'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('tasty',pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'running'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
